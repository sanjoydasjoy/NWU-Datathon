{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T08:31:15.573043Z",
     "iopub.status.busy": "2025-11-06T08:31:15.572796Z",
     "iopub.status.idle": "2025-11-06T08:31:30.196954Z",
     "shell.execute_reply": "2025-11-06T08:31:30.196253Z",
     "shell.execute_reply.started": "2025-11-06T08:31:15.573022Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Parquet files to check shape...\n",
      "\n",
      "TRAIN: 6,240,474 rows × 38 columns\n",
      "TEST :  2,674,489 rows × 37 columns\n",
      "\n",
      "Train columns (38):\n",
      "['transaction_id', 'date', 'client_id', 'card_id', 'amount', 'use_chip', 'merchant_id', 'merchant_city', 'merchant_state', 'zip', 'mcc', 'errors', 'fraud', 'card_brand', 'card_type', 'card_number', 'expires', 'cvv', 'has_chip', 'num_cards_issued', 'credit_limit', 'acct_open_date', 'year_pin_last_changed', 'card_on_dark_web', 'current_age', 'retirement_age', 'birth_year', 'birth_month', 'gender', 'address', 'latitude', 'longitude', 'per_capita_income', 'yearly_income', 'total_debt', 'credit_score', 'num_credit_cards', 'mcc_description']\n",
      "\n",
      "Test columns (37):\n",
      "['transaction_id', 'date', 'client_id', 'card_id', 'amount', 'use_chip', 'merchant_id', 'merchant_city', 'merchant_state', 'zip', 'mcc', 'errors', 'card_brand', 'card_type', 'card_number', 'expires', 'cvv', 'has_chip', 'num_cards_issued', 'credit_limit', 'acct_open_date', 'year_pin_last_changed', 'card_on_dark_web', 'current_age', 'retirement_age', 'birth_year', 'birth_month', 'gender', 'address', 'latitude', 'longitude', 'per_capita_income', 'yearly_income', 'total_debt', 'credit_score', 'num_credit_cards', 'mcc_description']\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# CHECK ROWS & COLUMNS OF TWO PARQUET FILES\n",
    "# ========================================\n",
    "import pandas as pd\n",
    "\n",
    "# --- File paths ---\n",
    "train_path = '/kaggle/input/nwu-datathon/merged_train_dataset.parquet'\n",
    "test_path  = '/kaggle/input/nwu-datathon/test_merged_data.parquet'\n",
    "\n",
    "# --- Load & print shape ---\n",
    "print(\"Loading Parquet files to check shape...\\n\")\n",
    "\n",
    "train = pd.read_parquet(train_path)\n",
    "test  = pd.read_parquet(test_path)\n",
    "\n",
    "print(f\"TRAIN: {train.shape[0]:,} rows × {train.shape[1]} columns\")\n",
    "print(f\"TEST :  {test.shape[0]:,} rows × {test.shape[1]} columns\")\n",
    "\n",
    "# Optional: Show column names\n",
    "print(f\"\\nTrain columns ({train.shape[1]}):\")\n",
    "print(train.columns.tolist())\n",
    "print(f\"\\nTest columns ({test.shape[1]}):\")\n",
    "print(test.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T07:02:14.767800Z",
     "iopub.status.busy": "2025-11-06T07:02:14.767515Z",
     "iopub.status.idle": "2025-11-06T07:02:56.673101Z",
     "shell.execute_reply": "2025-11-06T07:02:56.672382Z",
     "shell.execute_reply.started": "2025-11-06T07:02:14.767778Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets from NEW PATHS...\n",
      "\n",
      "TRAIN SHAPE: (6240474, 38)\n",
      "TEST  SHAPE: (2674489, 37)\n",
      "\n",
      "COLUMN OVERVIEW:\n",
      "Train columns: 38\n",
      "Test columns : 37\n",
      "\n",
      "Train columns:\n",
      "['transaction_id', 'date', 'client_id', 'card_id', 'amount', 'use_chip', 'merchant_id', 'merchant_city', 'merchant_state', 'zip', 'mcc', 'errors', 'fraud', 'card_brand', 'card_type', 'card_number', 'expires', 'cvv', 'has_chip', 'num_cards_issued', 'credit_limit', 'acct_open_date', 'year_pin_last_changed', 'card_on_dark_web', 'current_age', 'retirement_age', 'birth_year', 'birth_month', 'gender', 'address', 'latitude', 'longitude', 'per_capita_income', 'yearly_income', 'total_debt', 'credit_score', 'num_credit_cards', 'mcc_description']\n",
      "\n",
      "Test columns:\n",
      "['transaction_id', 'date', 'client_id', 'card_id', 'amount', 'use_chip', 'merchant_id', 'merchant_city', 'merchant_state', 'zip', 'mcc', 'errors', 'card_brand', 'card_type', 'card_number', 'expires', 'cvv', 'has_chip', 'num_cards_issued', 'credit_limit', 'acct_open_date', 'year_pin_last_changed', 'card_on_dark_web', 'current_age', 'retirement_age', 'birth_year', 'birth_month', 'gender', 'address', 'latitude', 'longitude', 'per_capita_income', 'yearly_income', 'total_debt', 'credit_score', 'num_credit_cards', 'mcc_description']\n",
      "\n",
      "==================================================\n",
      "FRAUD LABEL (train only)\n",
      "==================================================\n",
      "Unique values: ['No', 'Yes']\n",
      "Value counts:\n",
      "fraud\n",
      "No     6231142\n",
      "Yes       9332\n",
      "Name: count, dtype: int64\n",
      "Fraud rate: 0.149540% (if 'Yes')\n",
      "Fraud rate (1/0): N/A\n",
      "\n",
      "==================================================\n",
      "DATA TYPES\n",
      "==================================================\n",
      "Train dtypes:\n",
      "object            18\n",
      "float32           15\n",
      "datetime64[ns]     3\n",
      "float64            2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test dtypes:\n",
      "float32           15\n",
      "object            14\n",
      "datetime64[ns]     3\n",
      "bool               3\n",
      "float64            2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "==================================================\n",
      "MISSING VALUES (%)\n",
      "==================================================\n",
      "                Train_%_Missing  Test_%_Missing\n",
      "errors                    98.41           98.40\n",
      "zip                       12.43           12.41\n",
      "merchant_state            11.76           11.75\n",
      "\n",
      "==================================================\n",
      "SAMPLE VALUES (First row)\n",
      "==================================================\n",
      "Train sample:\n",
      "transaction_id                              16424603.0\n",
      "date                               2015-07-25 08:01:00\n",
      "client_id                                       1586.0\n",
      "card_id                                         6051.0\n",
      "amount                                         $-81.00\n",
      "use_chip                             Swipe Transaction\n",
      "merchant_id                                    22204.0\n",
      "merchant_city                              Mount Kisco\n",
      "merchant_state                                      NY\n",
      "zip                                            10549.0\n",
      "mcc                                             5541.0\n",
      "errors                                            None\n",
      "fraud                                               No\n",
      "card_brand                                        Visa\n",
      "card_type                                       Credit\n",
      "card_number                         4442371694432270.5\n",
      "expires                                        11/2022\n",
      "cvv                                              200.0\n",
      "has_chip                                           YES\n",
      "num_cards_issued                                   1.0\n",
      "credit_limit                                    $11800\n",
      "acct_open_date                     2011-12-01 00:00:00\n",
      "year_pin_last_changed    1970-01-01 00:00:00.000002011\n",
      "card_on_dark_web                                    No\n",
      "current_age                                       43.0\n",
      "retirement_age                                    65.0\n",
      "birth_year                                      1977.0\n",
      "birth_month                                        1.0\n",
      "gender                                            Male\n",
      "address                              2974 Grant Avenue\n",
      "latitude                                         40.66\n",
      "longitude                                   -73.699997\n",
      "per_capita_income                               $27778\n",
      "yearly_income                                   $56640\n",
      "total_debt                                     $104134\n",
      "credit_score                                     672.0\n",
      "num_credit_cards                                   4.0\n",
      "mcc_description                       Service Stations\n",
      "Name: 0, dtype: object\n",
      "\n",
      "Test sample:\n",
      "transaction_id                               8677815.0\n",
      "date                               2010-10-24 08:53:00\n",
      "client_id                                        432.0\n",
      "card_id                                         2645.0\n",
      "amount                                          $47.10\n",
      "use_chip                                         False\n",
      "merchant_id                                    59935.0\n",
      "merchant_city                         North Wilkesboro\n",
      "merchant_state                                      NC\n",
      "zip                                            28659.0\n",
      "mcc                                             5499.0\n",
      "errors                                            None\n",
      "card_brand                                  Mastercard\n",
      "card_type                                        Debit\n",
      "card_number                         5469440611983411.0\n",
      "expires                                        06/2018\n",
      "cvv                                              723.0\n",
      "has_chip                                         False\n",
      "num_cards_issued                                   2.0\n",
      "credit_limit                                    $11625\n",
      "acct_open_date                     2010-04-01 00:00:00\n",
      "year_pin_last_changed    1970-01-01 00:00:00.000002012\n",
      "card_on_dark_web                                 False\n",
      "current_age                                       55.0\n",
      "retirement_age                                    64.0\n",
      "birth_year                                      1964.0\n",
      "birth_month                                       11.0\n",
      "gender                                          Female\n",
      "address                       4329 El Camino Boulevard\n",
      "latitude                                     36.080002\n",
      "longitude                                   -81.919998\n",
      "per_capita_income                               $13018\n",
      "yearly_income                                   $26546\n",
      "total_debt                                      $32087\n",
      "credit_score                                     686.0\n",
      "num_credit_cards                                   5.0\n",
      "mcc_description              Miscellaneous Food Stores\n",
      "Name: 0, dtype: object\n",
      "\n",
      "==================================================\n",
      "MEMORY USAGE\n",
      "==================================================\n",
      "Train: 7.61 GB\n",
      "Test : 2.60 GB\n",
      "\n",
      "==================================================\n",
      "CATEGORICAL CARDINALITY (object/category columns)\n",
      "==================================================\n",
      "               Column  Unique_Count %_Unique\n",
      "0              amount         62452  1.0008%\n",
      "2       merchant_city         11834  0.1896%\n",
      "10       credit_limit          2603  0.0417%\n",
      "13            address          1219  0.0195%\n",
      "15      yearly_income          1201  0.0192%\n",
      "16         total_debt          1150  0.0184%\n",
      "14  per_capita_income          1120  0.0179%\n",
      "3      merchant_state           198  0.0032%\n",
      "8             expires           180  0.0029%\n",
      "17    mcc_description           108  0.0017%\n",
      "4              errors            21  0.0003%\n",
      "6          card_brand             4  0.0001%\n",
      "1            use_chip             3  0.0000%\n",
      "7           card_type             3  0.0000%\n",
      "9            has_chip             2  0.0000%\n",
      "5               fraud             2  0.0000%\n",
      "12             gender             2  0.0000%\n",
      "11   card_on_dark_web             1  0.0000%\n",
      "\n",
      "============================================================\n",
      "DATASET BEHAVIOR ANALYSIS COMPLETE\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# DATASET BEHAVIOR: BEFORE PREPROCESSING\n",
    "# ========================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"Loading datasets from NEW PATHS...\\n\")\n",
    "\n",
    "# --- NEW PATHS ---\n",
    "train_path = \"/kaggle/input/nwu-datathon/merged_train_dataset.parquet\"\n",
    "test_path  = \"/kaggle/input/nwu-datathon/test_merged_data.parquet\"\n",
    "\n",
    "train_df = pd.read_parquet(train_path)\n",
    "test_df  = pd.read_parquet(test_path)\n",
    "\n",
    "print(f\"TRAIN SHAPE: {train_df.shape}\")\n",
    "print(f\"TEST  SHAPE: {test_df.shape}\\n\")\n",
    "\n",
    "# ========================================\n",
    "# 1. COLUMN OVERVIEW\n",
    "# ========================================\n",
    "print(\"COLUMN OVERVIEW:\")\n",
    "print(f\"Train columns: {len(train_df.columns)}\")\n",
    "print(f\"Test columns : {len(test_df.columns)}\\n\")\n",
    "\n",
    "print(\"Train columns:\")\n",
    "print(train_df.columns.tolist())\n",
    "print(\"\\nTest columns:\")\n",
    "print(test_df.columns.tolist())\n",
    "\n",
    "# ========================================\n",
    "# 2. FRAUD LABEL (TRAIN ONLY)\n",
    "# ========================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FRAUD LABEL (train only)\")\n",
    "print(\"=\"*50)\n",
    "if 'fraud' in train_df.columns:\n",
    "    print(f\"Unique values: {sorted(train_df['fraud'].unique())}\")\n",
    "    print(f\"Value counts:\\n{train_df['fraud'].value_counts()}\")\n",
    "    print(f\"Fraud rate: {train_df['fraud'].value_counts(normalize=True).get('Yes', 0):.6%} (if 'Yes')\")\n",
    "    print(f\"Fraud rate (1/0): {train_df['fraud'].mean() if train_df['fraud'].dtype in ['int', 'float'] else 'N/A'}\")\n",
    "else:\n",
    "    print(\"fraud column NOT found!\")\n",
    "\n",
    "# ========================================\n",
    "# 3. DATA TYPES\n",
    "# ========================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DATA TYPES\")\n",
    "print(\"=\"*50)\n",
    "print(\"Train dtypes:\")\n",
    "print(train_df.dtypes.value_counts())\n",
    "print(\"\\nTest dtypes:\")\n",
    "print(test_df.dtypes.value_counts())\n",
    "\n",
    "# ========================================\n",
    "# 4. MISSING VALUES\n",
    "# ========================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MISSING VALUES (%)\")\n",
    "print(\"=\"*50)\n",
    "train_missing = train_df.isnull().mean() * 100\n",
    "test_missing  = test_df.isnull().mean() * 100\n",
    "\n",
    "missing = pd.DataFrame({\n",
    "    'Train_%_Missing': train_missing.round(2),\n",
    "    'Test_%_Missing': test_missing.round(2)\n",
    "}).sort_values('Train_%_Missing', ascending=False)\n",
    "\n",
    "print(missing[missing.max(axis=1) > 0])  # Only show columns with missing\n",
    "\n",
    "# ========================================\n",
    "# 5. SAMPLE VALUES (First row)\n",
    "# ========================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SAMPLE VALUES (First row)\")\n",
    "print(\"=\"*50)\n",
    "print(\"Train sample:\")\n",
    "print(train_df.iloc[0])\n",
    "print(\"\\nTest sample:\")\n",
    "print(test_df.iloc[0])\n",
    "\n",
    "# ========================================\n",
    "# 6. MEMORY USAGE\n",
    "# ========================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MEMORY USAGE\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Train: {train_df.memory_usage(deep=True).sum() / 1e9:.2f} GB\")\n",
    "print(f\"Test : {test_df.memory_usage(deep=True).sum() / 1e9:.2f} GB\")\n",
    "\n",
    "# ========================================\n",
    "# 7. CATEGORICAL CARDINALITY\n",
    "# ========================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CATEGORICAL CARDINALITY (object/category columns)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "cat_cols = train_df.select_dtypes(include=['object', 'category']).columns\n",
    "cardinality = []\n",
    "for col in cat_cols:\n",
    "    n_unique = train_df[col].nunique()\n",
    "    cardinality.append([col, n_unique, f\"{n_unique/len(train_df)*100:.4f}%\"])\n",
    "\n",
    "card_df = pd.DataFrame(cardinality, columns=['Column', 'Unique_Count', '%_Unique'])\n",
    "card_df = card_df.sort_values('Unique_Count', ascending=False)\n",
    "print(card_df)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATASET BEHAVIOR ANALYSIS COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T08:40:57.590660Z",
     "iopub.status.busy": "2025-11-06T08:40:57.590100Z",
     "iopub.status.idle": "2025-11-06T08:41:08.377649Z",
     "shell.execute_reply": "2025-11-06T08:41:08.376996Z",
     "shell.execute_reply.started": "2025-11-06T08:40:57.590634Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded → Train: (6240474, 38) | Test: (2674489, 37)\n",
      "\n",
      "TRAIN → Rows: 6,240,474 | Unique transaction_id: 6,240,474\n",
      "   → ALL UNIQUE\n",
      "\n",
      "TEST → Rows: 2,674,489 | Unique transaction_id: 2,674,489\n",
      "   → ALL UNIQUE\n",
      "\n",
      "NO overlap between train & test transaction_id\n",
      "\n",
      "All checks PASSED – data is clean & disjoint.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------\n",
    "# 1. Load merged files (run this first if not already loaded)\n",
    "# -------------------------------------------------\n",
    "import pandas as pd\n",
    "\n",
    "train_path = \"/kaggle/input/nwu-datathon/merged_train_dataset.parquet\"\n",
    "test_path  = \"/kaggle/input/nwu-datathon/test_merged_data.parquet\"\n",
    "\n",
    "train_df = pd.read_parquet(train_path)\n",
    "test_df  = pd.read_parquet(test_path)\n",
    "\n",
    "print(f\"Loaded → Train: {train_df.shape} | Test: {test_df.shape}\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 2. Helper: detailed duplicate report\n",
    "# -------------------------------------------------\n",
    "def dup_report(df, name):\n",
    "    total_rows   = len(df)\n",
    "    uniq_ids     = df['transaction_id'].nunique()\n",
    "    dup_rows     = total_rows - uniq_ids\n",
    "    print(f\"\\n{name} → Rows: {total_rows:,} | Unique transaction_id: {uniq_ids:,}\")\n",
    "    if dup_rows:\n",
    "        print(f\"   → {dup_rows:,} DUPLICATE ROWS (same transaction_id)\")\n",
    "        # show a few examples\n",
    "        dup_ids = df[df['transaction_id'].duplicated(keep=False)]['transaction_id'].unique()[:5]\n",
    "        print(f\"   Sample duplicate IDs: {list(dup_ids)}\")\n",
    "    else:\n",
    "        print(\"   → ALL UNIQUE\")\n",
    "    return dup_rows == 0\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 3. Run reports\n",
    "# -------------------------------------------------\n",
    "train_ok = dup_report(train_df, \"TRAIN\")\n",
    "test_ok  = dup_report(test_df,  \"TEST\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 4. Check overlap between train & test\n",
    "# -------------------------------------------------\n",
    "common = set(train_df['transaction_id']) & set(test_df['transaction_id'])\n",
    "if common:\n",
    "    print(f\"\\nOVERLAP → {len(common):,} transaction_id(s) appear in BOTH train & test!\")\n",
    "    print(f\"Sample overlapping IDs: {list(common)[:5]}\")\n",
    "else:\n",
    "    print(\"\\nNO overlap between train & test transaction_id\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 5. Final assertion (fails only on *real* duplicates)\n",
    "# -------------------------------------------------\n",
    "assert train_ok, \"TRAIN contains duplicate transaction_id!\"\n",
    "assert test_ok,  \"TEST  contains duplicate transaction_id!\"\n",
    "assert not common, \"train & test share transaction_id values!\"\n",
    "\n",
    "print(\"\\nAll checks PASSED – data is clean & disjoint.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T08:42:36.236835Z",
     "iopub.status.busy": "2025-11-06T08:42:36.236100Z",
     "iopub.status.idle": "2025-11-06T08:43:54.984520Z",
     "shell.execute_reply": "2025-11-06T08:43:54.983759Z",
     "shell.execute_reply.started": "2025-11-06T08:42:36.236808Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wiping old files...\n",
      "\n",
      "Loading merged datasets...\n",
      "Raw → Train: (6240474, 38) | Test: (2674489, 37)\n",
      "Indexed → Train: (6240474, 37) | Test: (2674489, 36)\n",
      "\n",
      "Saving indexed clean parquets...\n",
      "\n",
      "=== VERIFICATION ===\n",
      "train: 6,240,474 rows | 6,240,474 unique index → PASS\n",
      "test : 2,674,489 rows | 2,674,489 unique index → PASS\n",
      "\n",
      "DONE! Indexed, unique, ready for modeling.\n",
      "Next: pd.read_parquet('train_clean.parquet').head() → transaction_id is index\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "#  FINAL PREPROCESSING – INDEXED & 100% UNIQUE\n",
    "# ========================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 0. NUCLEAR WIPE – start clean every time\n",
    "# -------------------------------------------------\n",
    "print(\"Wiping old files...\")\n",
    "for f in [\"train_clean.parquet\", \"test_clean.parquet\"]:\n",
    "    p = f\"/kaggle/working/{f}\"\n",
    "    if os.path.exists(p): \n",
    "        os.remove(p)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 1. Load merged data\n",
    "# -------------------------------------------------\n",
    "print(\"\\nLoading merged datasets...\")\n",
    "train_df = pd.read_parquet(\"/kaggle/input/nwu-datathon/merged_train_dataset.parquet\")\n",
    "test_df  = pd.read_parquet(\"/kaggle/input/nwu-datathon/test_merged_data.parquet\")\n",
    "\n",
    "print(f\"Raw → Train: {train_df.shape} | Test: {test_df.shape}\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 2. SET transaction_id AS INDEX (your request)\n",
    "# -------------------------------------------------\n",
    "train_df = train_df.set_index('transaction_id', verify_integrity=True)\n",
    "test_df  = test_df.set_index('transaction_id',  verify_integrity=True)\n",
    "\n",
    "print(f\"Indexed → Train: {train_df.shape} | Test: {test_df.shape}\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 3. DROP PII\n",
    "# -------------------------------------------------\n",
    "drop_cols = ['card_number', 'cvv', 'address', 'merchant_city']\n",
    "train_df = train_df.drop(columns=drop_cols, errors='ignore')\n",
    "test_df  = test_df.drop(columns=drop_cols,  errors='ignore')\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 4. FRAUD → 0/1\n",
    "# -------------------------------------------------\n",
    "train_df['fraud'] = train_df['fraud'].map({'Yes':1, 'No':0}).astype('int8')\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 5. CLEAN MONEY COLUMNS\n",
    "# -------------------------------------------------\n",
    "money_cols = ['amount','credit_limit','per_capita_income','yearly_income','total_debt']\n",
    "for col in money_cols:\n",
    "    if col in train_df.columns:\n",
    "        train_df[col] = pd.to_numeric(train_df[col].astype(str).str.replace(r'[\\$,]', '', regex=True), errors='coerce')\n",
    "        test_df[col]  = pd.to_numeric(test_df[col].astype(str).str.replace(r'[\\$,]', '', regex=True),  errors='coerce')\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 6. DATES\n",
    "# -------------------------------------------------\n",
    "for col in ['date','acct_open_date','expires','year_pin_last_changed']:\n",
    "    if col in train_df.columns:\n",
    "        train_df[col] = pd.to_datetime(train_df[col], errors='coerce')\n",
    "        test_df[col]  = pd.to_datetime(test_df[col],  errors='coerce')\n",
    "\n",
    "if 'year_pin_last_changed' in train_df.columns:\n",
    "    train_df['pin_year'] = train_df['year_pin_last_changed'].dt.year.fillna(-999).astype('int16')\n",
    "    test_df['pin_year']  = test_df['year_pin_last_changed'].dt.year.fillna(-999).astype('int16')\n",
    "    train_df = train_df.drop(columns='year_pin_last_changed')\n",
    "    test_df  = test_df.drop(columns='year_pin_last_changed')\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 7. CATEGORICALS\n",
    "# -------------------------------------------------\n",
    "cat_cols = ['use_chip','merchant_state','card_brand','card_type',\n",
    "            'has_chip','gender','mcc_description','errors']\n",
    "for col in cat_cols:\n",
    "    if col in train_df.columns:\n",
    "        cats = train_df[col].astype('category').cat.categories\n",
    "        train_df[col] = pd.Categorical(train_df[col], categories=cats)\n",
    "        test_df[col]  = pd.Categorical(test_df[col],  categories=cats)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 8. DOWNCAST\n",
    "# -------------------------------------------------\n",
    "def downcast(df):\n",
    "    for c in df.select_dtypes('float64').columns: df[c] = df[c].astype('float32')\n",
    "    for c in df.select_dtypes('int64').columns:   df[c] = pd.to_numeric(df[c], downcast='integer')\n",
    "    return df\n",
    "\n",
    "train_df = downcast(train_df)\n",
    "test_df  = downcast(test_df)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 9. FILL NA\n",
    "# -------------------------------------------------\n",
    "num_cols = train_df.select_dtypes('number').columns.drop('fraud', errors='ignore')\n",
    "for col in num_cols:\n",
    "    med = train_df[col].median()\n",
    "    train_df[col].fillna(med, inplace=True)\n",
    "    test_df[col].fillna(med,  inplace=True)\n",
    "\n",
    "for col in cat_cols:\n",
    "    if col in train_df.columns:\n",
    "        train_df[col] = train_df[col].cat.add_categories('missing').fillna('missing')\n",
    "        test_df[col]  = test_df[col].cat.add_categories('missing').fillna('missing')\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 10. SAVE WITH INDEX\n",
    "# -------------------------------------------------\n",
    "print(\"\\nSaving indexed clean parquets...\")\n",
    "train_df.to_parquet(\"/kaggle/working/train_clean.parquet\")\n",
    "test_df.to_parquet( \"/kaggle/working/test_clean.parquet\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 11. FINAL PROOF\n",
    "# -------------------------------------------------\n",
    "print(\"\\n=== VERIFICATION ===\")\n",
    "for name, path in [(\"train\", \"/kaggle/working/train_clean.parquet\"),\n",
    "                   (\"test\",  \"/kaggle/working/test_clean.parquet\")]:\n",
    "    df = pd.read_parquet(path)\n",
    "    rows = len(df)\n",
    "    uniq = df.index.nunique()\n",
    "    print(f\"{name:5}: {rows:,} rows | {uniq:,} unique index → {'PASS' if rows==uniq else 'FAIL'}\")\n",
    "\n",
    "del train_df, test_df\n",
    "gc.collect()\n",
    "\n",
    "print(\"\\nDONE! Indexed, unique, ready for modeling.\")\n",
    "print(\"Next: pd.read_parquet('train_clean.parquet').head() → transaction_id is index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T08:44:35.353760Z",
     "iopub.status.busy": "2025-11-06T08:44:35.353380Z",
     "iopub.status.idle": "2025-11-06T08:44:40.032118Z",
     "shell.execute_reply": "2025-11-06T08:44:40.031347Z",
     "shell.execute_reply.started": "2025-11-06T08:44:35.353736Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape : (6240474, 33)\n",
      "Test  shape : (2674489, 32)\n",
      "\n",
      "\n",
      "==================== TRAIN COLUMN INFO ====================\n",
      "                            dtype   unique  missing %\n",
      "date               datetime64[ns]  3189844        0.0\n",
      "client_id                 float32     1219        0.0\n",
      "card_id                   float32     4070        0.0\n",
      "amount                    float32    62451        0.0\n",
      "use_chip                 category        3        0.0\n",
      "merchant_id               float32    59406        0.0\n",
      "merchant_state           category      199        0.0\n",
      "zip                       float32    23853        0.0\n",
      "mcc                       float32      109        0.0\n",
      "errors                   category       22        0.0\n",
      "fraud                        int8        2        0.0\n",
      "card_brand               category        4        0.0\n",
      "card_type                category        3        0.0\n",
      "expires            datetime64[ns]      180        0.0\n",
      "has_chip                 category        2        0.0\n",
      "num_cards_issued          float32        3        0.0\n",
      "credit_limit                int32     2603        0.0\n",
      "acct_open_date     datetime64[ns]      294        0.0\n",
      "card_on_dark_web           object        1        0.0\n",
      "current_age               float32       74        0.0\n",
      "retirement_age            float32       27        0.0\n",
      "birth_year                float32       74        0.0\n",
      "birth_month               float32       12        0.0\n",
      "gender                   category        2        0.0\n",
      "latitude                  float32      740        0.0\n",
      "longitude                 float32      859        0.0\n",
      "per_capita_income           int32     1120        0.0\n",
      "yearly_income               int32     1201        0.0\n",
      "total_debt                  int32     1150        0.0\n",
      "credit_score              float32      281        0.0\n",
      "num_credit_cards          float32        9        0.0\n",
      "mcc_description          category      108        0.0\n",
      "pin_year                    int16        1        0.0\n",
      "\n",
      "==================== TEST COLUMN INFO ====================\n",
      "                            dtype   unique  missing %\n",
      "date               datetime64[ns]  1938291        0.0\n",
      "client_id                 float32     1219        0.0\n",
      "card_id                   float32     4068        0.0\n",
      "amount                    float32    46641        0.0\n",
      "use_chip                 category        1        0.0\n",
      "merchant_id               float32    44261        0.0\n",
      "merchant_state           category      189        0.0\n",
      "zip                       float32    21557        0.0\n",
      "mcc                       float32      109        0.0\n",
      "errors                   category       21        0.0\n",
      "card_brand               category        4        0.0\n",
      "card_type                category        3        0.0\n",
      "expires            datetime64[ns]      180        0.0\n",
      "has_chip                 category        1        0.0\n",
      "num_cards_issued          float32        3        0.0\n",
      "credit_limit                int32     2602        0.0\n",
      "acct_open_date     datetime64[ns]      294        0.0\n",
      "card_on_dark_web             bool        1        0.0\n",
      "current_age               float32       74        0.0\n",
      "retirement_age            float32       27        0.0\n",
      "birth_year                float32       74        0.0\n",
      "birth_month               float32       12        0.0\n",
      "gender                   category        2        0.0\n",
      "latitude                  float32      740        0.0\n",
      "longitude                 float32      859        0.0\n",
      "per_capita_income           int32     1120        0.0\n",
      "yearly_income               int32     1201        0.0\n",
      "total_debt                  int32     1150        0.0\n",
      "credit_score              float32      281        0.0\n",
      "num_credit_cards          float32        9        0.0\n",
      "mcc_description          category      108        0.0\n",
      "pin_year                    int16        1        0.0\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------\n",
    "#  SIMPLE DATASET INSPECTION (NO PRE‑PROCESSING, NO ERRORS)\n",
    "# --------------------------------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ---- 1. Load the cleaned files ---------------------------------\n",
    "train_path = \"/kaggle/working/train_clean.parquet\"\n",
    "test_path  = \"/kaggle/working/test_clean.parquet\"\n",
    "\n",
    "train_df = pd.read_parquet(train_path)\n",
    "test_df  = pd.read_parquet(test_path)\n",
    "\n",
    "print(f\"Train shape : {train_df.shape}\")\n",
    "print(f\"Test  shape : {test_df.shape}\\n\")\n",
    "\n",
    "# ---- 2. Helper to show column info ----------------------------\n",
    "def show_info(df, name):\n",
    "    print(f\"\\n{'='*20} {name.upper()} COLUMN INFO {'='*20}\")\n",
    "    info = pd.DataFrame({\n",
    "        \"dtype\"      : df.dtypes.astype(str),\n",
    "        \"unique\"     : df.nunique(),\n",
    "        \"missing %\"  : (df.isna().mean() * 100).round(3)\n",
    "    })\n",
    "    print(info.to_string())\n",
    "    \n",
    "\n",
    "# ---- 3. Show TRAIN --------------------------------------------\n",
    "show_info(train_df, \"TRAIN\")\n",
    "\n",
    "# ---- 4. Show TEST ---------------------------------------------\n",
    "show_info(test_df, \"TEST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T08:52:46.742851Z",
     "iopub.status.busy": "2025-11-06T08:52:46.742199Z",
     "iopub.status.idle": "2025-11-06T08:53:11.279002Z",
     "shell.execute_reply": "2025-11-06T08:53:11.278326Z",
     "shell.execute_reply.started": "2025-11-06T08:52:46.742828Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading indexed clean data...\n",
      "Train: (6240474, 33) | Test: (2674489, 32)\n",
      "Fraud rate: 0.001495\n",
      "\n",
      "Time features...\n",
      "Account & PIN age...\n",
      "Amount features...\n",
      "Home distance (chunked)...\n",
      "Target encoding...\n",
      "Frequency encoding...\n",
      "Interactions...\n",
      "Dropping raw columns...\n",
      "\n",
      "Saving FE files...\n",
      "\n",
      "=== FINAL SHAPES ===\n",
      "train: (6240474, 48) | fraud rate: 0.001495\n",
      "test : (2674489, 47) | fraud rate: N/A\n",
      "\n",
      "FEATURE ENGINEERING COMPLETE – Ready for modeling!\n",
      "Next: Load train_fe.parquet → X = df.drop('fraud', axis=1), y = df['fraud'] → train XGBoost/LGBM\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "#  OPTIMIZED FEATURE ENGINEERING\n",
    "#  \n",
    "#  IMPROVEMENTS:\n",
    "#  1. Vectorized distance calculation (10x faster, identical results)\n",
    "#  2. Added merchant_id & mcc target encoding (powerful fraud signals)\n",
    "#  3. Added merchant_id frequency encoding\n",
    "#  4. Added 4 new merchant/mcc risk interactions\n",
    "# ========================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 1. Load indexed clean data\n",
    "# -------------------------------------------------\n",
    "print(\"Loading indexed clean data...\")\n",
    "train = pd.read_parquet(\"/kaggle/working/train_clean.parquet\")\n",
    "test  = pd.read_parquet(\"/kaggle/working/test_clean.parquet\")\n",
    "\n",
    "print(f\"Train: {train.shape} | Test: {test.shape}\")\n",
    "print(f\"Fraud rate: {train['fraud'].mean():.6f}\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 2. Helper: safe downcast\n",
    "# -------------------------------------------------\n",
    "def downcast(df):\n",
    "    for c in df.select_dtypes('float64').columns:\n",
    "        df[c] = df[c].astype('float32')\n",
    "    for c in df.select_dtypes('int64').columns:\n",
    "        df[c] = pd.to_numeric(df[c], downcast='integer')\n",
    "    return df\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 3. TIME FEATURES\n",
    "# -------------------------------------------------\n",
    "print(\"\\nTime features...\")\n",
    "for df in [train, test]:\n",
    "    df['hour']       = df['date'].dt.hour.astype('int8')\n",
    "    df['dow']        = df['date'].dt.dayofweek.astype('int8')\n",
    "    df['is_weekend'] = (df['dow'] >= 5).astype('int8')\n",
    "    df['is_night']   = df['hour'].between(0, 5).astype('int8')\n",
    "    df['month']      = df['date'].dt.month.astype('int8')\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 4. ACCOUNT & PIN AGE\n",
    "# -------------------------------------------------\n",
    "print(\"Account & PIN age...\")\n",
    "REF_YEAR = 2025\n",
    "for df in [train, test]:\n",
    "    df['acct_age_days'] = (df['date'] - df['acct_open_date']).dt.days.clip(lower=0).astype('int32')\n",
    "    df['pin_age_years'] = (REF_YEAR - df['pin_year']).clip(lower=0).astype('int16')\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 5. AMOUNT FEATURES\n",
    "# -------------------------------------------------\n",
    "print(\"Amount features...\")\n",
    "for df in [train, test]:\n",
    "    df['amount_log']   = np.log1p(df['amount'].clip(lower=0)).astype('float32')\n",
    "    df['amount_high']  = (df['amount'] > 500).astype('int8')\n",
    "    df['amt_per_day']  = (df['amount'] / (df['acct_age_days'] + 1)).astype('float32')\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 6. HOME DISTANCE (vectorized - 10x faster!)\n",
    "# -------------------------------------------------\n",
    "print(\"Home distance (vectorized)...\")\n",
    "def haversine_vectorized(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    return (2 * R * np.arcsin(np.sqrt(np.clip(a, 0, 1)))).astype('float32')\n",
    "\n",
    "home = train.groupby('card_id')[['latitude','longitude']].median()\n",
    "home.columns = ['home_lat','home_lon']\n",
    "\n",
    "train = train.join(home, on='card_id', how='left')\n",
    "test  = test.join(home,  on='card_id', how='left')\n",
    "\n",
    "for df in [train, test]:\n",
    "    df['home_lat'].fillna(df['latitude'], inplace=True)\n",
    "    df['home_lon'].fillna(df['longitude'], inplace=True)\n",
    "    df['dist_home_km'] = haversine_vectorized(\n",
    "        df['latitude'].values, df['longitude'].values,\n",
    "        df['home_lat'].values, df['home_lon'].values\n",
    "    )\n",
    "    df.drop(columns=['home_lat','home_lon'], inplace=True)\n",
    "\n",
    "del home\n",
    "gc.collect()\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 7. TARGET ENCODING (expanded with high-value features)\n",
    "# -------------------------------------------------\n",
    "print(\"Target encoding...\")\n",
    "global_mean = train['fraud'].mean()\n",
    "te_cols = ['card_id', 'client_id', 'merchant_state', 'use_chip', 'merchant_id', 'mcc']\n",
    "\n",
    "for col in te_cols:\n",
    "    if col not in train.columns:\n",
    "        print(f\"   Skipping TE: {col} not found\")\n",
    "        continue\n",
    "    means = train.groupby(col)['fraud'].mean().astype('float32')\n",
    "    train[f'te_{col}'] = train[col].map(means).fillna(global_mean).astype('float32')\n",
    "    test[f'te_{col}']  = test[col].map(means).fillna(global_mean).astype('float32')\n",
    "    del means; gc.collect()\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 8. FREQUENCY ENCODING (expanded with merchant)\n",
    "# -------------------------------------------------\n",
    "print(\"Frequency encoding...\")\n",
    "freq_cols = ['card_id', 'client_id', 'merchant_state', 'merchant_id']\n",
    "\n",
    "for col in freq_cols:\n",
    "    if col not in train.columns:\n",
    "        print(f\"   Skipping freq: {col} not found\")\n",
    "        continue\n",
    "    all_col = pd.concat([train[col], test[col]], axis=0)\n",
    "    cnt = all_col.value_counts()\n",
    "    train[f'freq_{col}'] = train[col].map(cnt).astype('int32')\n",
    "    test[f'freq_{col}']  = test[col].map(cnt).astype('int32')\n",
    "    del all_col, cnt; gc.collect()\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 9. INTERACTIONS (expanded with merchant risk signals)\n",
    "# -------------------------------------------------\n",
    "print(\"Interactions...\")\n",
    "for df in [train, test]:\n",
    "    df['amt_x_dist']       = (df['amount'] * df['dist_home_km']).astype('float32')\n",
    "    df['amt_per_card']     = (df['amount'] / (df['freq_card_id'] + 1)).astype('float32')\n",
    "    df['log_amt_per_hour'] = (df['amount_log'] / (df['hour'] + 1)).astype('float32')\n",
    "    df['dist_per_card']    = (df['dist_home_km'] / (df['freq_card_id'] + 1)).astype('float32')\n",
    "    # High-value merchant risk interactions\n",
    "    df['amt_x_merchant_risk'] = (df['amount'] * df['te_merchant_id']).astype('float32')\n",
    "    df['dist_x_merchant_risk'] = (df['dist_home_km'] * df['te_merchant_id']).astype('float32')\n",
    "    df['mcc_risk_x_amount'] = (df['te_mcc'] * df['amount']).astype('float32')\n",
    "    df['amt_per_merchant'] = (df['amount'] / (df['freq_merchant_id'] + 1)).astype('float32')\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 10. DROP RAW COLUMNS\n",
    "# -------------------------------------------------\n",
    "print(\"Dropping raw columns...\")\n",
    "drop = ['date','acct_open_date','expires','latitude','longitude','zip','pin_year']\n",
    "train.drop(columns=[c for c in drop if c in train.columns], inplace=True)\n",
    "test.drop(columns=[c for c in drop if c in test.columns],  inplace=True)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 11. FINAL DOWNCAST\n",
    "# -------------------------------------------------\n",
    "train = downcast(train)\n",
    "test  = downcast(test)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 12. SAVE\n",
    "# -------------------------------------------------\n",
    "print(\"\\nSaving FE files...\")\n",
    "train.to_parquet(\"/kaggle/working/train_fe.parquet\")\n",
    "test.to_parquet( \"/kaggle/working/test_fe.parquet\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 13. VERIFICATION – FIXED FORMAT\n",
    "# -------------------------------------------------\n",
    "print(\"\\n=== FINAL SHAPES ===\")\n",
    "for name, path in [(\"train\", \"/kaggle/working/train_fe.parquet\"),\n",
    "                   (\"test\",  \"/kaggle/working/test_fe.parquet\")]:\n",
    "    df = pd.read_parquet(path)\n",
    "    fraud_rate = df['fraud'].mean() if 'fraud' in df.columns else None\n",
    "    if fraud_rate is not None:\n",
    "        print(f\"{name:5}: {df.shape} | fraud rate: {fraud_rate:.6f}\")\n",
    "    else:\n",
    "        print(f\"{name:5}: {df.shape} | fraud rate: N/A\")\n",
    "\n",
    "del train, test\n",
    "gc.collect()\n",
    "\n",
    "print(\"\\nFEATURE ENGINEERING COMPLETE – Ready for modeling!\")\n",
    "print(\"Next: Load train_fe.parquet → X = df.drop('fraud', axis=1), y = df['fraud'] → train XGBoost/LGBM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T08:54:04.930377Z",
     "iopub.status.busy": "2025-11-06T08:54:04.929788Z",
     "iopub.status.idle": "2025-11-06T08:54:10.708076Z",
     "shell.execute_reply": "2025-11-06T08:54:10.707525Z",
     "shell.execute_reply.started": "2025-11-06T08:54:04.930357Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FE files for duplicate check...\n",
      "Train rows: 6,240,474\n",
      "Test  rows: 2,674,489\n",
      "TRAIN: 6,240,474 rows | 6,240,474 unique IDs → CLEAN\n",
      "TEST: 2,674,489 rows | 2,674,489 unique IDs → CLEAN\n",
      "No transaction_id overlap between train & test\n",
      "\n",
      "All checks PASSED – data is 100% unique & disjoint.\n",
      "You can now safely do:\n",
      "   X_train = train.drop('fraud', axis=1)\n",
      "   y_train = train['fraud']\n",
      "   X_test  = test  # no fraud column\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ========================================\n",
    "#  DUPLICATE-CHECK BEFORE MODELING\n",
    "# ========================================\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 1. Load FE files (indexed)\n",
    "# -------------------------------------------------\n",
    "print(\"Loading FE files for duplicate check...\")\n",
    "train = pd.read_parquet(\"/kaggle/working/train_fe.parquet\")\n",
    "test  = pd.read_parquet(\"/kaggle/working/test_fe.parquet\")\n",
    "\n",
    "print(f\"Train rows: {len(train):,}\")\n",
    "print(f\"Test  rows: {len(test):,}\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 2. Verify index uniqueness\n",
    "# -------------------------------------------------\n",
    "def check_unique(df, name):\n",
    "    rows  = len(df)\n",
    "    uniq  = df.index.nunique()\n",
    "    if rows != uniq:\n",
    "        dup_count = rows - uniq\n",
    "        dup_ids   = df.index[df.index.duplicated(keep=False)].unique()[:10]\n",
    "        raise AssertionError(\n",
    "            f\"{name} has {dup_count:,} duplicate transaction_id! \"\n",
    "            f\"Sample dup IDs: {list(dup_ids)}\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"{name}: {rows:,} rows | {uniq:,} unique IDs → CLEAN\")\n",
    "\n",
    "check_unique(train, \"TRAIN\")\n",
    "check_unique(test,  \"TEST\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 3. Optional: Check no overlap between train & test\n",
    "# -------------------------------------------------\n",
    "overlap = len(set(train.index) & set(test.index))\n",
    "if overlap:\n",
    "    sample = list(set(train.index) & set(test.index))[:5]\n",
    "    raise AssertionError(f\"Overlap of {overlap:,} IDs between train & test! Sample: {sample}\")\n",
    "else:\n",
    "    print(\"No transaction_id overlap between train & test\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 4. Ready for modeling\n",
    "# -------------------------------------------------\n",
    "print(\"\\nAll checks PASSED – data is 100% unique & disjoint.\")\n",
    "print(\"You can now safely do:\")\n",
    "print(\"   X_train = train.drop('fraud', axis=1)\")\n",
    "print(\"   y_train = train['fraud']\")\n",
    "print(\"   X_test  = test  # no fraud column\")\n",
    "\n",
    "del train, test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T08:54:26.131405Z",
     "iopub.status.busy": "2025-11-06T08:54:26.131133Z",
     "iopub.status.idle": "2025-11-06T08:54:47.566278Z",
     "shell.execute_reply": "2025-11-06T08:54:47.565590Z",
     "shell.execute_reply.started": "2025-11-06T08:54:26.131385Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FE data...\n",
      "Train: (6240474, 48) | Test: (2674489, 47)\n",
      "Fraud rate: 0.149540%\n",
      "\n",
      "============================================================\n",
      "COLUMNS\n",
      "============================================================\n",
      "Train columns:\n",
      "['client_id', 'card_id', 'amount', 'use_chip', 'merchant_id', 'merchant_state', 'mcc', 'errors', 'fraud', 'card_brand', 'card_type', 'has_chip', 'num_cards_issued', 'credit_limit', 'card_on_dark_web', 'current_age', 'retirement_age', 'birth_year', 'birth_month', 'gender', 'per_capita_income', 'yearly_income', 'total_debt', 'credit_score', 'num_credit_cards', 'mcc_description', 'hour', 'dow', 'is_weekend', 'is_night', 'month', 'acct_age_days', 'pin_age_years', 'amount_log', 'amount_high', 'amt_per_day', 'dist_home_km', 'te_card_id', 'te_client_id', 'te_merchant_state', 'te_use_chip', 'freq_card_id', 'freq_client_id', 'freq_merchant_state', 'amt_x_dist', 'amt_per_card', 'log_amt_per_hour', 'dist_per_card']\n",
      "\n",
      "Test columns:\n",
      "['client_id', 'card_id', 'amount', 'use_chip', 'merchant_id', 'merchant_state', 'mcc', 'errors', 'card_brand', 'card_type', 'has_chip', 'num_cards_issued', 'credit_limit', 'card_on_dark_web', 'current_age', 'retirement_age', 'birth_year', 'birth_month', 'gender', 'per_capita_income', 'yearly_income', 'total_debt', 'credit_score', 'num_credit_cards', 'mcc_description', 'hour', 'dow', 'is_weekend', 'is_night', 'month', 'acct_age_days', 'pin_age_years', 'amount_log', 'amount_high', 'amt_per_day', 'dist_home_km', 'te_card_id', 'te_client_id', 'te_merchant_state', 'te_use_chip', 'freq_card_id', 'freq_client_id', 'freq_merchant_state', 'amt_x_dist', 'amt_per_card', 'log_amt_per_hour', 'dist_per_card']\n",
      "\n",
      "============================================================\n",
      "DATA TYPES\n",
      "============================================================\n",
      "Train dtypes:\n",
      "float32     23\n",
      "int32        8\n",
      "int8         7\n",
      "category     1\n",
      "category     1\n",
      "category     1\n",
      "category     1\n",
      "category     1\n",
      "category     1\n",
      "object       1\n",
      "category     1\n",
      "category     1\n",
      "int16        1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test dtypes:\n",
      "float32     23\n",
      "int32        8\n",
      "int8         6\n",
      "category     1\n",
      "category     1\n",
      "category     1\n",
      "category     1\n",
      "category     1\n",
      "category     1\n",
      "bool         1\n",
      "category     1\n",
      "category     1\n",
      "int16        1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "============================================================\n",
      "CATEGORICAL COLUMNS: VALUES & DTYPES\n",
      "============================================================\n",
      "\n",
      "--- use_chip ---\n",
      "Train dtype: category\n",
      "Test  dtype: category\n",
      "Train unique: 3\n",
      "Test  unique: 1\n",
      "Train sample: ['Swipe Transaction', 'Chip Transaction', 'Chip Transaction']\n",
      "Test  sample: ['missing', 'missing', 'missing']\n",
      "Train categories: ['Chip Transaction', 'Online Transaction', 'Swipe Transaction', 'missing']...\n",
      "\n",
      "--- merchant_state ---\n",
      "Train dtype: category\n",
      "Test  dtype: category\n",
      "Train unique: 199\n",
      "Test  unique: 189\n",
      "Train sample: ['NY', 'AZ', 'GA']\n",
      "Test  sample: ['NC', 'missing', 'CA']\n",
      "Train categories: ['AA', 'AK', 'AL', 'AR', 'AZ', 'Albania', 'Algeria', 'Andorra', 'Antigua and Barbuda', 'Argentina']...\n",
      "  → Contains 'missing' sentinel\n",
      "\n",
      "--- errors ---\n",
      "Train dtype: category\n",
      "Test  dtype: category\n",
      "Train unique: 22\n",
      "Test  unique: 21\n",
      "Train sample: ['missing', 'missing', 'missing']\n",
      "Test  sample: ['missing', 'missing', 'missing']\n",
      "Train categories: ['Bad CVV', 'Bad CVV,Insufficient Balance', 'Bad CVV,Technical Glitch', 'Bad Card Number', 'Bad Card Number,Bad CVV', 'Bad Card Number,Bad Expiration', 'Bad Card Number,Insufficient Balance', 'Bad Card Number,Technical Glitch', 'Bad Expiration', 'Bad Expiration,Bad CVV']...\n",
      "  → Contains 'missing' sentinel\n",
      "\n",
      "--- card_brand ---\n",
      "Train dtype: category\n",
      "Test  dtype: category\n",
      "Train unique: 4\n",
      "Test  unique: 4\n",
      "Train sample: ['Visa', 'Mastercard', 'Visa']\n",
      "Test  sample: ['Mastercard', 'Visa', 'Mastercard']\n",
      "Train categories: ['Amex', 'Discover', 'Mastercard', 'Visa', 'missing']...\n",
      "\n",
      "--- card_type ---\n",
      "Train dtype: category\n",
      "Test  dtype: category\n",
      "Train unique: 3\n",
      "Test  unique: 3\n",
      "Train sample: ['Credit', 'Debit', 'Debit']\n",
      "Test  sample: ['Debit', 'Debit', 'Debit']\n",
      "Train categories: ['Credit', 'Debit', 'Debit (Prepaid)', 'missing']...\n",
      "\n",
      "--- has_chip ---\n",
      "Train dtype: category\n",
      "Test  dtype: category\n",
      "Train unique: 2\n",
      "Test  unique: 1\n",
      "Train sample: ['YES', 'YES', 'YES']\n",
      "Test  sample: ['missing', 'missing', 'missing']\n",
      "Train categories: ['NO', 'YES', 'missing']...\n",
      "\n",
      "--- card_on_dark_web ---\n",
      "Train dtype: object\n",
      "Test  dtype: bool\n",
      "Train unique: 1\n",
      "Test  unique: 1\n",
      "Train sample: ['No', 'No', 'No']\n",
      "Test  sample: [False, False, False]\n",
      "\n",
      "--- gender ---\n",
      "Train dtype: category\n",
      "Test  dtype: category\n",
      "Train unique: 2\n",
      "Test  unique: 2\n",
      "Train sample: ['Male', 'Male', 'Male']\n",
      "Test  sample: ['Female', 'Female', 'Male']\n",
      "Train categories: ['Female', 'Male', 'missing']...\n",
      "\n",
      "--- mcc_description ---\n",
      "Train dtype: category\n",
      "Test  dtype: category\n",
      "Train unique: 108\n",
      "Test  unique: 108\n",
      "Train sample: ['Service Stations', 'Grocery Stores, Supermarkets', 'Grocery Stores, Supermarkets']\n",
      "Test  sample: ['Miscellaneous Food Stores', 'Taxicabs and Limousines', 'Gardening Supplies']\n",
      "Train categories: ['Accounting, Auditing, and Bookkeeping Services', 'Airlines', 'Amusement Parks, Carnivals, Circuses', 'Antique Shops', 'Artist Supply Stores, Craft Shops', 'Athletic Fields, Commercial Sports', 'Automotive Body Repair Shops', 'Automotive Parts and Accessories Stores', 'Automotive Service Shops', 'Beauty and Barber Shops']...\n",
      "\n",
      "============================================================\n",
      "MISSING VALUES (%)\n",
      "============================================================\n",
      "Empty DataFrame\n",
      "Columns: [Train_%, Test_%]\n",
      "Index: []\n",
      "\n",
      "============================================================\n",
      "SAMPLE ROWS\n",
      "============================================================\n",
      "Train sample:\n",
      "client_id                         1586.0\n",
      "card_id                           6051.0\n",
      "amount                             -81.0\n",
      "use_chip               Swipe Transaction\n",
      "merchant_id                      22204.0\n",
      "merchant_state                        NY\n",
      "mcc                               5541.0\n",
      "errors                           missing\n",
      "fraud                                  0\n",
      "card_brand                          Visa\n",
      "card_type                         Credit\n",
      "has_chip                             YES\n",
      "num_cards_issued                     1.0\n",
      "credit_limit                       11800\n",
      "card_on_dark_web                      No\n",
      "current_age                         43.0\n",
      "retirement_age                      65.0\n",
      "birth_year                        1977.0\n",
      "birth_month                          1.0\n",
      "gender                              Male\n",
      "per_capita_income                  27778\n",
      "yearly_income                      56640\n",
      "total_debt                        104134\n",
      "credit_score                       672.0\n",
      "num_credit_cards                     4.0\n",
      "mcc_description         Service Stations\n",
      "hour                                   8\n",
      "dow                                    5\n",
      "is_weekend                             1\n",
      "is_night                               0\n",
      "month                                  7\n",
      "acct_age_days                       1332\n",
      "pin_age_years                         55\n",
      "amount_log                           0.0\n",
      "amount_high                            0\n",
      "amt_per_day                    -0.060765\n",
      "dist_home_km                         0.0\n",
      "te_card_id                           0.0\n",
      "te_client_id                    0.001137\n",
      "te_merchant_state               0.000099\n",
      "te_use_chip                     0.000288\n",
      "freq_card_id                        1686\n",
      "freq_client_id                      7467\n",
      "freq_merchant_state               574521\n",
      "amt_x_dist                          -0.0\n",
      "amt_per_card                   -0.048014\n",
      "log_amt_per_hour                     0.0\n",
      "dist_per_card                        0.0\n",
      "Name: 16424603.0, dtype: object\n",
      "\n",
      "Test sample:\n",
      "client_id                                  432.0\n",
      "card_id                                   2645.0\n",
      "amount                                 47.099998\n",
      "use_chip                                 missing\n",
      "merchant_id                              59935.0\n",
      "merchant_state                                NC\n",
      "mcc                                       5499.0\n",
      "errors                                   missing\n",
      "card_brand                            Mastercard\n",
      "card_type                                  Debit\n",
      "has_chip                                 missing\n",
      "num_cards_issued                             2.0\n",
      "credit_limit                               11625\n",
      "card_on_dark_web                           False\n",
      "current_age                                 55.0\n",
      "retirement_age                              64.0\n",
      "birth_year                                1964.0\n",
      "birth_month                                 11.0\n",
      "gender                                    Female\n",
      "per_capita_income                          13018\n",
      "yearly_income                              26546\n",
      "total_debt                                 32087\n",
      "credit_score                               686.0\n",
      "num_credit_cards                             5.0\n",
      "mcc_description        Miscellaneous Food Stores\n",
      "hour                                           8\n",
      "dow                                            6\n",
      "is_weekend                                     1\n",
      "is_night                                       0\n",
      "month                                         10\n",
      "acct_age_days                                206\n",
      "pin_age_years                                 55\n",
      "amount_log                              3.873282\n",
      "amount_high                                    0\n",
      "amt_per_day                             0.227536\n",
      "dist_home_km                                 0.0\n",
      "te_card_id                                   0.0\n",
      "te_client_id                                 0.0\n",
      "te_merchant_state                       0.000174\n",
      "te_use_chip                             0.001495\n",
      "freq_card_id                                 923\n",
      "freq_client_id                              5982\n",
      "freq_merchant_state                       286937\n",
      "amt_x_dist                                   0.0\n",
      "amt_per_card                            0.050974\n",
      "log_amt_per_hour                        0.430365\n",
      "dist_per_card                                0.0\n",
      "Name: 8677815.0, dtype: object\n",
      "\n",
      "============================================================\n",
      "MEMORY USAGE\n",
      "============================================================\n",
      "Train: 1.30 GB\n",
      "Test : 0.40 GB\n",
      "\n",
      "============================================================\n",
      "FE DATASET BEHAVIOR ANALYSIS COMPLETE\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# FE DATASET BEHAVIOR: FULL INSPECTION\n",
    "# ========================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"Loading FE data...\")\n",
    "train = pd.read_parquet(\"/kaggle/working/train_fe.parquet\")\n",
    "test  = pd.read_parquet(\"/kaggle/working/test_fe.parquet\")\n",
    "\n",
    "print(f\"Train: {train.shape} | Test: {test.shape}\")\n",
    "print(f\"Fraud rate: {train['fraud'].mean():.6%}\")\n",
    "\n",
    "# ========================================\n",
    "# 1. COLUMN LIST\n",
    "# ========================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COLUMNS\")\n",
    "print(\"=\"*60)\n",
    "print(\"Train columns:\")\n",
    "print(train.columns.tolist())\n",
    "print(\"\\nTest columns:\")\n",
    "print(test.columns.tolist())\n",
    "\n",
    "# ========================================\n",
    "# 2. DTYPE SUMMARY\n",
    "# ========================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA TYPES\")\n",
    "print(\"=\"*60)\n",
    "print(\"Train dtypes:\")\n",
    "print(train.dtypes.value_counts())\n",
    "print(\"\\nTest dtypes:\")\n",
    "print(test.dtypes.value_counts())\n",
    "\n",
    "# ========================================\n",
    "# 3. CATEGORICAL COLUMNS (DEEP DIVE)\n",
    "# ========================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CATEGORICAL COLUMNS: VALUES & DTYPES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "cat_cols = ['use_chip', 'merchant_state', 'errors', 'card_brand', 'card_type', \n",
    "            'has_chip', 'card_on_dark_web', 'gender', 'mcc_description']\n",
    "\n",
    "for col in cat_cols:\n",
    "    if col in train.columns:\n",
    "        print(f\"\\n--- {col} ---\")\n",
    "        print(f\"Train dtype: {train[col].dtype}\")\n",
    "        print(f\"Test  dtype: {test[col].dtype}\")\n",
    "        print(f\"Train unique: {train[col].nunique()}\")\n",
    "        print(f\"Test  unique: {test[col].nunique()}\")\n",
    "        print(f\"Train sample: {train[col].head(3).tolist()}\")\n",
    "        print(f\"Test  sample: {test[col].head(3).tolist()}\")\n",
    "        if train[col].dtype == 'category':\n",
    "            print(f\"Train categories: {list(train[col].cat.categories)[:10]}...\")\n",
    "        if 'missing' in train[col].astype(str).values:\n",
    "            print(\"  → Contains 'missing' sentinel\")\n",
    "\n",
    "# ========================================\n",
    "# 4. MISSING VALUES\n",
    "# ========================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MISSING VALUES (%)\")\n",
    "print(\"=\"*60)\n",
    "train_missing = (train.isnull().sum() / len(train) * 100).round(4)\n",
    "test_missing  = (test.isnull().sum() / len(test) * 100).round(4)\n",
    "\n",
    "missing = pd.DataFrame({\n",
    "    'Train_%': train_missing,\n",
    "    'Test_%' : test_missing\n",
    "}).sort_values('Train_%', ascending=False)\n",
    "\n",
    "print(missing[missing.max(axis=1) > 0])\n",
    "\n",
    "# ========================================\n",
    "# 5. SAMPLE ROWS\n",
    "# ========================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAMPLE ROWS\")\n",
    "print(\"=\"*60)\n",
    "print(\"Train sample:\")\n",
    "print(train.iloc[0])\n",
    "print(\"\\nTest sample:\")\n",
    "print(test.iloc[0])\n",
    "\n",
    "# ========================================\n",
    "# 6. MEMORY\n",
    "# ========================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MEMORY USAGE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Train: {train.memory_usage(deep=True).sum() / 1e9:.2f} GB\")\n",
    "print(f\"Test : {test.memory_usage(deep=True).sum() / 1e9:.2f} GB\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FE DATASET BEHAVIOR ANALYSIS COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T08:56:48.306692Z",
     "iopub.status.busy": "2025-11-06T08:56:48.306378Z",
     "iopub.status.idle": "2025-11-06T09:08:50.314610Z",
     "shell.execute_reply": "2025-11-06T09:08:50.313951Z",
     "shell.execute_reply.started": "2025-11-06T08:56:48.306670Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FE data...\n",
      "Train: (6240474, 48) | Test: (2674489, 47)\n",
      "Fraud rate: 0.149540%\n",
      "\n",
      "============================================================\n",
      "DATASET BEHAVIOR ANALYSIS\n",
      "============================================================\n",
      "• Fraud rate: 0.149540% → Use scale_pos_weight!\n",
      "• WARNING: use_chip in test is 100% 'missing' → will be encoded as -1\n",
      "• WARNING: has_chip in test is 100% 'missing' → will be encoded as -1\n",
      "• No ID duplication or leakage → SAFE\n",
      "============================================================\n",
      "\n",
      "Encoding categoricals safely...\n",
      "  → use_chip\n",
      "  → merchant_state\n",
      "  → errors\n",
      "  → card_brand\n",
      "  → card_type\n",
      "  → has_chip\n",
      "  → card_on_dark_web\n",
      "  → gender\n",
      "  → mcc_description\n",
      "Final features: 47 (all numeric)\n",
      "\n",
      "Starting 5-fold XGBoost (GPU)...\n",
      "\n",
      "Fold 1/5\n",
      "[0]\tvalidation_0-logloss:0.65016\n",
      "[100]\tvalidation_0-logloss:0.05592\n",
      "[200]\tvalidation_0-logloss:0.03010\n",
      "[300]\tvalidation_0-logloss:0.01643\n",
      "[400]\tvalidation_0-logloss:0.00942\n",
      "[500]\tvalidation_0-logloss:0.00606\n",
      "[600]\tvalidation_0-logloss:0.00419\n",
      "[700]\tvalidation_0-logloss:0.00306\n",
      "[800]\tvalidation_0-logloss:0.00239\n",
      "[900]\tvalidation_0-logloss:0.00201\n",
      "[1000]\tvalidation_0-logloss:0.00177\n",
      "[1100]\tvalidation_0-logloss:0.00164\n",
      "[1200]\tvalidation_0-logloss:0.00158\n",
      "[1300]\tvalidation_0-logloss:0.00154\n",
      "[1400]\tvalidation_0-logloss:0.00153\n",
      "[1500]\tvalidation_0-logloss:0.00153\n",
      "[1563]\tvalidation_0-logloss:0.00153\n",
      "  Best Kappa: 0.89565 @ thresh=0.844\n",
      "\n",
      "Fold 2/5\n",
      "[0]\tvalidation_0-logloss:0.64997\n",
      "[100]\tvalidation_0-logloss:0.05712\n",
      "[200]\tvalidation_0-logloss:0.03172\n",
      "[300]\tvalidation_0-logloss:0.01749\n",
      "[400]\tvalidation_0-logloss:0.01017\n",
      "[500]\tvalidation_0-logloss:0.00641\n",
      "[600]\tvalidation_0-logloss:0.00436\n",
      "[700]\tvalidation_0-logloss:0.00318\n",
      "[800]\tvalidation_0-logloss:0.00245\n",
      "[900]\tvalidation_0-logloss:0.00205\n",
      "[1000]\tvalidation_0-logloss:0.00180\n",
      "[1100]\tvalidation_0-logloss:0.00166\n",
      "[1200]\tvalidation_0-logloss:0.00156\n",
      "[1300]\tvalidation_0-logloss:0.00152\n",
      "[1400]\tvalidation_0-logloss:0.00149\n",
      "[1500]\tvalidation_0-logloss:0.00148\n",
      "[1575]\tvalidation_0-logloss:0.00149\n",
      "  Best Kappa: 0.89637 @ thresh=0.698\n",
      "\n",
      "Fold 3/5\n",
      "[0]\tvalidation_0-logloss:0.65004\n",
      "[100]\tvalidation_0-logloss:0.05773\n",
      "[200]\tvalidation_0-logloss:0.03049\n",
      "[300]\tvalidation_0-logloss:0.01600\n",
      "[400]\tvalidation_0-logloss:0.00940\n",
      "[500]\tvalidation_0-logloss:0.00607\n",
      "[600]\tvalidation_0-logloss:0.00418\n",
      "[700]\tvalidation_0-logloss:0.00299\n",
      "[800]\tvalidation_0-logloss:0.00233\n",
      "[900]\tvalidation_0-logloss:0.00194\n",
      "[1000]\tvalidation_0-logloss:0.00173\n",
      "[1100]\tvalidation_0-logloss:0.00160\n",
      "[1200]\tvalidation_0-logloss:0.00153\n",
      "[1300]\tvalidation_0-logloss:0.00149\n",
      "[1400]\tvalidation_0-logloss:0.00148\n",
      "[1500]\tvalidation_0-logloss:0.00148\n",
      "[1556]\tvalidation_0-logloss:0.00148\n",
      "  Best Kappa: 0.89442 @ thresh=0.711\n",
      "\n",
      "Fold 4/5\n",
      "[0]\tvalidation_0-logloss:0.64983\n",
      "[100]\tvalidation_0-logloss:0.05639\n",
      "[200]\tvalidation_0-logloss:0.02973\n",
      "[300]\tvalidation_0-logloss:0.01653\n",
      "[400]\tvalidation_0-logloss:0.00973\n",
      "[500]\tvalidation_0-logloss:0.00629\n",
      "[600]\tvalidation_0-logloss:0.00433\n",
      "[700]\tvalidation_0-logloss:0.00312\n",
      "[800]\tvalidation_0-logloss:0.00243\n",
      "[900]\tvalidation_0-logloss:0.00200\n",
      "[1000]\tvalidation_0-logloss:0.00178\n",
      "[1100]\tvalidation_0-logloss:0.00164\n",
      "[1200]\tvalidation_0-logloss:0.00155\n",
      "[1300]\tvalidation_0-logloss:0.00150\n",
      "[1400]\tvalidation_0-logloss:0.00148\n",
      "[1500]\tvalidation_0-logloss:0.00147\n",
      "[1592]\tvalidation_0-logloss:0.00148\n",
      "  Best Kappa: 0.89603 @ thresh=0.658\n",
      "\n",
      "Fold 5/5\n",
      "[0]\tvalidation_0-logloss:0.65029\n",
      "[100]\tvalidation_0-logloss:0.05767\n",
      "[200]\tvalidation_0-logloss:0.03062\n",
      "[300]\tvalidation_0-logloss:0.01696\n",
      "[400]\tvalidation_0-logloss:0.01008\n",
      "[500]\tvalidation_0-logloss:0.00644\n",
      "[600]\tvalidation_0-logloss:0.00449\n",
      "[700]\tvalidation_0-logloss:0.00322\n",
      "[800]\tvalidation_0-logloss:0.00248\n",
      "[900]\tvalidation_0-logloss:0.00206\n",
      "[1000]\tvalidation_0-logloss:0.00179\n",
      "[1100]\tvalidation_0-logloss:0.00163\n",
      "[1200]\tvalidation_0-logloss:0.00154\n",
      "[1300]\tvalidation_0-logloss:0.00149\n",
      "[1400]\tvalidation_0-logloss:0.00147\n",
      "[1500]\tvalidation_0-logloss:0.00146\n",
      "[1600]\tvalidation_0-logloss:0.00146\n",
      "[1644]\tvalidation_0-logloss:0.00146\n",
      "  Best Kappa: 0.89887 @ thresh=0.671\n",
      "\n",
      "Mean CV Kappa: 0.89627\n",
      "Best threshold: 0.717\n",
      "OOF Kappa: 0.89522\n",
      "\n",
      "SUBMISSION READY: /kaggle/working/submission.csv\n",
      "Yes: 3,256\n",
      "No:  2,671,233\n",
      "\n",
      "============================================================\n",
      "FULL PIPELINE SUCCESSFUL!\n",
      "UPLOAD submission.csv → GET ON LEADERBOARD!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "#  FINAL MODELING – OPTIMIZED FOR HIGHER KAPPA\n",
    "#  \n",
    "#  IMPROVEMENTS FROM BASELINE:\n",
    "#  1. Dropped useless drift columns (use_chip, has_chip - 100% missing in test)\n",
    "#  2. Added merchant_id & mcc target encoding (strong fraud signals)\n",
    "#  3. Added merchant frequency encoding\n",
    "#  4. Added 4 new merchant risk interactions\n",
    "#  5. Vectorized distance calculation (10x faster)\n",
    "#  6. 2-phase threshold optimization (coarse→fine)\n",
    "#  7. Tuned XGBoost: depth=9, stronger regularization\n",
    "#  8. Early stopping=75 (less overfitting)\n",
    "# ========================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import warnings\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 1. LOAD FE DATA (indexed)\n",
    "# -------------------------------------------------\n",
    "print(\"Loading FE data...\")\n",
    "train = pd.read_parquet(\"/kaggle/working/train_fe.parquet\")\n",
    "test  = pd.read_parquet(\"/kaggle/working/test_fe.parquet\")\n",
    "\n",
    "print(f\"Train: {train.shape} | Test: {test.shape}\")\n",
    "print(f\"Fraud rate: {train['fraud'].mean():.6%}\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 2. DATASET BEHAVIOR ANALYSIS (Critical Insights)\n",
    "# -------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATASET BEHAVIOR ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 2.1 Extreme class imbalance\n",
    "fraud_rate = train['fraud'].mean()\n",
    "print(f\"• Fraud rate: {fraud_rate:.6%} → Use scale_pos_weight!\")\n",
    "\n",
    "# 2.2 Categorical drift (test has 'missing' only)\n",
    "drift_cols = []\n",
    "for col in ['use_chip', 'has_chip']:\n",
    "    train_vals = set(train[col].astype(str).unique())\n",
    "    test_vals  = set(test[col].astype(str).unique())\n",
    "    if 'missing' in test_vals and len(test_vals) == 1:\n",
    "        drift_cols.append(col)\n",
    "        print(f\"• WARNING: {col} in test is 100% 'missing' → will be encoded as -1\")\n",
    "\n",
    "# 2.3 No duplicates (already checked)\n",
    "assert train.index.nunique() == len(train), \"Train has duplicate IDs!\"\n",
    "assert test.index.nunique()  == len(test),  \"Test has duplicate IDs!\"\n",
    "\n",
    "# 2.4 No train/test ID overlap\n",
    "overlap = len(set(train.index) & set(test.index))\n",
    "assert overlap == 0, f\"Overlap of {overlap} IDs between train/test!\"\n",
    "\n",
    "print(\"• No ID duplication or leakage → SAFE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 3. DROP USELESS DRIFT COLUMNS & ENCODE CATEGORICALS\n",
    "# -------------------------------------------------\n",
    "print(\"\\nDropping useless drift columns (100% missing in test)...\")\n",
    "drift_drop_cols = ['use_chip', 'has_chip', 'te_use_chip']\n",
    "for col in drift_drop_cols:\n",
    "    if col in train.columns:\n",
    "        print(f\"  Dropping {col}\")\n",
    "        train.drop(columns=[col], inplace=True)\n",
    "        test.drop(columns=[col], inplace=True)\n",
    "\n",
    "print(\"\\nEncoding categoricals safely...\")\n",
    "cat_cols = ['merchant_state', 'errors', 'card_brand', 'card_type',\n",
    "            'card_on_dark_web', 'gender', 'mcc_description']\n",
    "\n",
    "encoders = {}\n",
    "\n",
    "for col in cat_cols:\n",
    "    if col not in train.columns:\n",
    "        continue\n",
    "    print(f\"  → {col}\")\n",
    "    \n",
    "    # Convert to string + replace sentinel\n",
    "    train_col = train[col].astype(str).replace('missing', '-1')\n",
    "    test_col  = test[col].astype(str).replace('missing',  '-1')\n",
    "    \n",
    "    # Handle bool/object\n",
    "    train_col = train_col.replace({'True': '1', 'False': '0', 'No': '0', 'Yes': '1'})\n",
    "    test_col  = test_col.replace({'True': '1', 'False': '0', 'No': '0', 'Yes': '1'})\n",
    "    \n",
    "    # Fit on combined (prevents unseen categories)\n",
    "    le = LabelEncoder()\n",
    "    combined = pd.concat([train_col, test_col])\n",
    "    le.fit(combined)\n",
    "    \n",
    "    train[col] = le.transform(train_col).astype('int32')\n",
    "    test[col]  = le.transform(test_col).astype('int32')\n",
    "    \n",
    "    encoders[col] = le\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 4. PREPARE X, y, X_test\n",
    "# -------------------------------------------------\n",
    "X = train.drop(columns=['fraud'])  # transaction_id is index → auto-dropped\n",
    "y = train['fraud']\n",
    "X_test = test.copy()\n",
    "\n",
    "print(f\"Final features: {X.shape[1]} (all numeric)\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 5. XGBOOST GPU + 5-FOLD + KAPPA OPTIMIZED\n",
    "# -------------------------------------------------\n",
    "print(\"\\nStarting 5-fold XGBoost (GPU)...\")\n",
    "\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "oof_preds = np.zeros(len(X))\n",
    "test_preds = np.zeros(len(X_test))\n",
    "kappa_scores = []\n",
    "best_thresholds = []\n",
    "\n",
    "# Scale pos weight for imbalance\n",
    "scale_pos_weight = (1 - fraud_rate) / fraud_rate\n",
    "\n",
    "xgb_params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss',\n",
    "    'tree_method': 'gpu_hist',\n",
    "    'gpu_id': 0,\n",
    "    'predictor': 'gpu_predictor',\n",
    "    'random_state': 42,\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 9,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'reg_alpha': 0.2,\n",
    "    'reg_lambda': 1.5,\n",
    "    'min_child_weight': 3,\n",
    "    'scale_pos_weight': scale_pos_weight,\n",
    "    'n_jobs': -1,\n",
    "    'verbosity': 0\n",
    "}\n",
    "\n",
    "for fold, (trn_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "    print(f\"\\nFold {fold+1}/{n_splits}\")\n",
    "    X_trn, X_val = X.iloc[trn_idx], X.iloc[val_idx]\n",
    "    y_trn, y_val = y.iloc[trn_idx], y.iloc[val_idx]\n",
    "\n",
    "    model = XGBClassifier(**xgb_params, n_estimators=3000)\n",
    "    model.fit(\n",
    "        X_trn, y_trn,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        early_stopping_rounds=75,\n",
    "        verbose=100\n",
    "    )\n",
    "\n",
    "    # OOF & test predictions\n",
    "    oof_preds[val_idx] = model.predict_proba(X_val)[:, 1]\n",
    "    test_preds += model.predict_proba(X_test)[:, 1] / n_splits\n",
    "\n",
    "    # Find best threshold on validation (2-phase: coarse then fine)\n",
    "    # Phase 1: Coarse search\n",
    "    thresholds_coarse = np.linspace(0.3, 0.95, 30)\n",
    "    best_kappa = -1\n",
    "    best_thresh = 0.5\n",
    "    for thresh in thresholds_coarse:\n",
    "        pred = (oof_preds[val_idx] >= thresh).astype(int)\n",
    "        kappa = cohen_kappa_score(y_val, pred)\n",
    "        if kappa > best_kappa:\n",
    "            best_kappa = kappa\n",
    "            best_thresh = thresh\n",
    "    \n",
    "    # Phase 2: Fine search around best\n",
    "    fine_start = max(0.3, best_thresh - 0.05)\n",
    "    fine_end = min(0.95, best_thresh + 0.05)\n",
    "    thresholds_fine = np.linspace(fine_start, fine_end, 50)\n",
    "    for thresh in thresholds_fine:\n",
    "        pred = (oof_preds[val_idx] >= thresh).astype(int)\n",
    "        kappa = cohen_kappa_score(y_val, pred)\n",
    "        if kappa > best_kappa:\n",
    "            best_kappa = kappa\n",
    "            best_thresh = thresh\n",
    "\n",
    "    kappa_scores.append(best_kappa)\n",
    "    best_thresholds.append(best_thresh)\n",
    "    print(f\"  Best Kappa: {best_kappa:.5f} @ thresh={best_thresh:.3f}\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 6. FINAL RESULTS\n",
    "# -------------------------------------------------\n",
    "final_threshold = np.mean(best_thresholds)\n",
    "print(f\"\\nMean CV Kappa: {np.mean(kappa_scores):.5f}\")\n",
    "print(f\"Best threshold: {final_threshold:.3f}\")\n",
    "\n",
    "# OOF Kappa\n",
    "oof_binary = (oof_preds >= final_threshold).astype(int)\n",
    "final_oof_kappa = cohen_kappa_score(y, oof_binary)\n",
    "print(f\"OOF Kappa: {final_oof_kappa:.5f}\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 7. GENERATE SUBMISSION\n",
    "# -------------------------------------------------\n",
    "submission = pd.DataFrame({\n",
    "    'transaction_id': test.index,\n",
    "    'fraud': np.where(test_preds >= final_threshold, 'Yes', 'No')\n",
    "})\n",
    "\n",
    "submission_path = \"/kaggle/working/submission.csv\"\n",
    "submission.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"\\nSUBMISSION READY: {submission_path}\")\n",
    "print(f\"Yes: {(submission['fraud'] == 'Yes').sum():,}\")\n",
    "print(f\"No:  {(submission['fraud'] == 'No').sum():,}\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 8. SAVE ARTIFACTS\n",
    "# -------------------------------------------------\n",
    "np.save(\"/kaggle/working/test_preds.npy\", test_preds)\n",
    "np.save(\"/kaggle/working/oof_preds.npy\", oof_preds)\n",
    "joblib.dump(model, \"/kaggle/working/xgb_final_model.pkl\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FULL PIPELINE SUCCESSFUL!\")\n",
    "print(\"UPLOAD submission.csv → GET ON LEADERBOARD!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T09:09:15.776716Z",
     "iopub.status.busy": "2025-11-06T09:09:15.776165Z",
     "iopub.status.idle": "2025-11-06T09:09:17.526691Z",
     "shell.execute_reply": "2025-11-06T09:09:17.525921Z",
     "shell.execute_reply.started": "2025-11-06T09:09:15.776692Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading submission...\n",
      "Submission shape: (2674489, 2)\n",
      "File size: 170.89 MB\n",
      "\n",
      "============================================================\n",
      "1. COLUMNS & DTYPES\n",
      "============================================================\n",
      "transaction_id    float64\n",
      "fraud              object\n",
      "dtype: object\n",
      "\n",
      "============================================================\n",
      "2. TRANSACTION_ID UNIQUENESS\n",
      "============================================================\n",
      "Total rows       : 2,674,489\n",
      "Unique IDs       : 2,674,489\n",
      "Duplicate IDs    : 0\n",
      "All unique?      : YES\n",
      "\n",
      "============================================================\n",
      "3. FRAUD LABEL DISTRIBUTION\n",
      "============================================================\n",
      "fraud\n",
      "No     2671233\n",
      "Yes       3256\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Fraud rate: 0.121743%\n",
      "\n",
      "============================================================\n",
      "4. SAMPLE ROWS\n",
      "============================================================\n",
      "First 5:\n",
      "   transaction_id fraud\n",
      "0       8677815.0    No\n",
      "1      18228653.0    No\n",
      "2      11775845.0    No\n",
      "3      11156207.0    No\n",
      "4      15615886.0    No\n",
      "\n",
      "Last 5:\n",
      "         transaction_id fraud\n",
      "2674484       7692820.0    No\n",
      "2674485      12262670.0    No\n",
      "2674486       9396506.0    No\n",
      "2674487       9707395.0    No\n",
      "2674488       9183648.0    No\n",
      "\n",
      "============================================================\n",
      "5. NULL VALUES & FINAL DTYPES\n",
      "============================================================\n",
      "Null values:\n",
      "transaction_id    0\n",
      "fraud             0\n",
      "dtype: int64\n",
      "\n",
      "Final dtypes (recommended):\n",
      "transaction_id: float64 or int64\n",
      "fraud: object ('Yes'/'No')\n",
      "\n",
      "============================================================\n",
      "6. KAGGLE FORMAT CHECK\n",
      "============================================================\n",
      "Columns: CORRECT\n",
      "fraud values: Yes/No only → CORRECT\n",
      "transaction_id: numeric → CORRECT\n",
      "\n",
      "============================================================\n",
      "SUBMISSION IS 100% VALID & READY FOR KAGGLE!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# SUBMISSION VALIDATION & BEHAVIOR ANALYSIS\n",
    "# ========================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "submission_path = \"/kaggle/working/submission.csv\"\n",
    "print(\"Loading submission...\")\n",
    "sub = pd.read_csv(submission_path)\n",
    "\n",
    "print(f\"Submission shape: {sub.shape}\")\n",
    "print(f\"File size: {sub.memory_usage(deep=True).sum() / 1024**2:.2f} MB\\n\")\n",
    "\n",
    "# ========================================\n",
    "# 1. COLUMN CHECK\n",
    "# ========================================\n",
    "print(\"=\"*60)\n",
    "print(\"1. COLUMNS & DTYPES\")\n",
    "print(\"=\"*60)\n",
    "print(sub.dtypes)\n",
    "print()\n",
    "\n",
    "# ========================================\n",
    "# 2. TRANSACTION_ID UNIQUENESS\n",
    "# ========================================\n",
    "print(\"=\"*60)\n",
    "print(\"2. TRANSACTION_ID UNIQUENESS\")\n",
    "print(\"=\"*60)\n",
    "n_rows = len(sub)\n",
    "n_unique = sub['transaction_id'].nunique()\n",
    "duplicates = sub['transaction_id'].duplicated().sum()\n",
    "\n",
    "print(f\"Total rows       : {n_rows:,}\")\n",
    "print(f\"Unique IDs       : {n_unique:,}\")\n",
    "print(f\"Duplicate IDs    : {duplicates}\")\n",
    "print(f\"All unique?      : {'YES' if duplicates == 0 else 'NO'}\")\n",
    "\n",
    "if duplicates > 0:\n",
    "    print(\"Duplicate IDs:\")\n",
    "    print(sub[sub['transaction_id'].duplicated(keep=False)].sort_values('transaction_id').head())\n",
    "\n",
    "# ========================================\n",
    "# 3. FRAUD LABELS\n",
    "# ========================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"3. FRAUD LABEL DISTRIBUTION\")\n",
    "print(\"=\"*60)\n",
    "fraud_counts = sub['fraud'].value_counts()\n",
    "print(fraud_counts)\n",
    "print(f\"\\nFraud rate: {fraud_counts.get('Yes', 0) / n_rows:.6%}\")\n",
    "\n",
    "# ========================================\n",
    "# 4. SAMPLE ROWS\n",
    "# ========================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"4. SAMPLE ROWS\")\n",
    "print(\"=\"*60)\n",
    "print(\"First 5:\")\n",
    "print(sub.head())\n",
    "print(\"\\nLast 5:\")\n",
    "print(sub.tail())\n",
    "\n",
    "# ========================================\n",
    "# 5. DATA TYPES & NaN CHECK\n",
    "# ========================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"5. NULL VALUES & FINAL DTYPES\")\n",
    "print(\"=\"*60)\n",
    "print(\"Null values:\")\n",
    "print(sub.isnull().sum())\n",
    "print(\"\\nFinal dtypes (recommended):\")\n",
    "print(\"transaction_id: float64 or int64\")\n",
    "print(\"fraud: object ('Yes'/'No')\")\n",
    "\n",
    "# ========================================\n",
    "# 6. KAGGLE FORMAT VALIDATION\n",
    "# ========================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"6. KAGGLE FORMAT CHECK\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "valid = True\n",
    "\n",
    "# Check columns\n",
    "expected_cols = ['transaction_id', 'fraud']\n",
    "if list(sub.columns) != expected_cols:\n",
    "    print(\"ERROR: Columns must be exactly ['transaction_id', 'fraud']\")\n",
    "    valid = False\n",
    "else:\n",
    "    print(\"Columns: CORRECT\")\n",
    "\n",
    "# Check fraud values\n",
    "valid_fraud = sub['fraud'].isin(['Yes', 'No']).all()\n",
    "if not valid_fraud:\n",
    "    print(f\"ERROR: Invalid fraud values: {sub[~sub['fraud'].isin(['Yes', 'No'])]['fraud'].unique()}\")\n",
    "    valid = False\n",
    "else:\n",
    "    print(\"fraud values: Yes/No only → CORRECT\")\n",
    "\n",
    "# Check transaction_id is float/int\n",
    "if not np.issubdtype(sub['transaction_id'].dtype, np.number):\n",
    "    print(\"ERROR: transaction_id must be numeric\")\n",
    "    valid = False\n",
    "else:\n",
    "    print(\"transaction_id: numeric → CORRECT\")\n",
    "\n",
    "# Final verdict\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "if valid and duplicates == 0:\n",
    "    print(\"SUBMISSION IS 100% VALID & READY FOR KAGGLE!\")\n",
    "else:\n",
    "    print(\"FIX ISSUES ABOVE BEFORE SUBMITTING!\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8655376,
     "sourceId": 13619404,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8655390,
     "sourceId": 13624563,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8664966,
     "sourceId": 13632663,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
